{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945e9773-411e-4608-8a32-bed5e6d5f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 모델 클래스 선택\n",
    "2. 모델 초모수 선택\n",
    "3. 모델을 훈련 데이터에 적합\n",
    "4. 모델을 사용해서 새 데이터에 대한 레이블 예측\n",
    "\"\"\";\n",
    "\n",
    "# 처음 두 단계인 모델 선택과 초모수 선택이 도구와 기법을 효과적으로 사용하는 데 가장 중요한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0b55a0-d506-4a97-9f99-9072f35d6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 검증에 대한 고려사항\n",
    "\n",
    "# 잘못된 방식의 모델 검증\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# k-nearest neighbor (n_neighbors = 1)\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "model = KNN(n_neighbors=1)\n",
    "model.fit(X, y)\n",
    "y_model = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e40bca-a837-46f1-b437-fdd73f55d6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_model, y)\n",
    "\n",
    "# 100% => \"같은 데이터로 모델을 훈련하고 검증한다\" 라는 근본적인 오류\n",
    "# 100% => nearest neighbor model은 단순히 훈련 데이터를 저장하고 점들과 새로운 데이터 비교, 인스턴스 기반 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8acd1f-0145-4bb5-b43a-a51e6dad109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 올바른 방식의 모델 검증: 검정 표본\n",
    "# 검정 표본(holdout set)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 데이터를 각각 50%로 나눔\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size=0.5)\n",
    "\n",
    "# 모델을 이 가운데 하나의 데이터 집합에 적합\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 모델을 두 번째 데이터 집합으로 검증함\n",
    "y2_model = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05e0d97c-ccc7-4898-9144-1ad761172c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9066666666666666"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y2_model, y_test)\n",
    "# 90% 정확도 => 검정 표본은 알려지지 않은 데이터와 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b534f078-54e8-4202-9601-4ebc0d6af9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9066666666666666, 0.96)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 교차 검증을 통한 모델 검증\n",
    "# cross-validation => 데이터의 각 하위 집합이 훈련 자료와 검증 자료로 사용 되도록 일련의 적합\n",
    "\n",
    "X1, X2, Y1, Y2 = train_test_split(X, y, random_state=0, train_size=0.5) # 2겹 교차 검증\n",
    "\n",
    "y2_model = model.fit(X1, Y1).predict(X2)\n",
    "y1_model = model.fit(X2, Y2).predict(X1)\n",
    "\n",
    "accuracy_score(y2_model, Y2), accuracy_score(y1_model, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e2fef2-cf45-4f5d-ba27-c543130bd1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.93333333, 0.93333333, 1.        ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SciKit-Learn의 cross_val_score 루틴\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(model, X, y, cv=5) # 5겹 교차 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b58fd8-db2a-4717-a064-3ff9ca9773b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '_iter_test_masks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:822\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 822\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 극단적인 교차 검증 => 데이터 점의 개수와 같은 수만큼 반복\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 단일 관측치 제거 방식 (leave-one-out)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LeaveOneOut\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLeaveOneOut\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m scores\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:833\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    830\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    831\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[1;32m--> 833\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:86\u001b[0m, in \u001b[0;36mBaseCrossValidator.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m     84\u001b[0m X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m     85\u001b[0m indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(_num_samples(X))\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iter_test_masks\u001b[49m(X, y, groups):\n\u001b[0;32m     87\u001b[0m     train_index \u001b[38;5;241m=\u001b[39m indices[np\u001b[38;5;241m.\u001b[39mlogical_not(test_index)]\n\u001b[0;32m     88\u001b[0m     test_index \u001b[38;5;241m=\u001b[39m indices[test_index]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_iter_test_masks'"
     ]
    }
   ],
   "source": [
    "# 극단적인 교차 검증 => 데이터 점의 개수와 같은 수만큼 반복\n",
    "# 단일 관측치 제거 방식 (leave-one-out)\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "scores = cross_val_score(model, X, y, cv=LeaveOneOut)\n",
    "scores.mean() # ~96% 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e4aff-273e-4fcc-a0e2-f2a91218650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 모델 선택하기\n",
    "\"\"\"\n",
    "추정기의 성과가 저조하다면 어떻게 개선할 것인가?\n",
    "=> 더 복잡학허나 더 유연한 모델 사용\n",
    "=> 덜 복잡하거나 덜 유연한 모델 사용\n",
    "=> 더 많은 훈련 표본 수집\n",
    "=> 각 표본에 특징을 추가하기 위해 더 많은 데이터 수집\n",
    "\"\"\";\n",
    "\n",
    "# 편향-분산 트레이드오프\n",
    "# 편한과 분산 사이의 트레이드오프에서 가장 효율적인 점 찾기\n",
    "\n",
    "\"\"\"\n",
    "과소적합(underfit)\n",
    "=> 직선보다 더 복잡한 데이터를 직선모델로 설명 하기\n",
    "=> 데이터의 특징을 적절히 설명할 만큼 모델 유연성이 충분하지 않다\n",
    "\n",
    "과적합(overfit)\n",
    "=> 모델이 기본 데이터 분포와 함께 임의의 오류까지 설명할 정도로 너무 과한 모델 유연성을 가지고 있다\n",
    "=> 이 모델은 고분산을 가지고 있다\n",
    "\n",
    "표본 점수 (R-squared value)\n",
    "=> 모델이 대상값의 단순 평균에 비해 얼마나 잘 수행하는지 측정\n",
    "=> 1(완전히 일치함)\n",
    "=> 0(모델이 데이터의 단순 평균을 구하는 수준)\n",
    "=> 음수 (나쁜 모델)\n",
    "\"\"\";\n",
    "\n",
    "# 검증 곡선 (validation curve)\n",
    "\"\"\"\n",
    "1. 훈련 점수는 언제나 검증 점수보다 높다 (모델은 이미 본 데이터에서 더 잘 적합)\n",
    "2. 모델 복잡도가 너무 낮은 경우(고편향), 데이터가 과소적합(underfit) => 훈련 / 테스트 데이터 둘다 예측 못 함\n",
    "3. 모델 복잡도가 너무 높은 경우(고분산), 데이터가 과적합(overfit) => 훈련 데이터 예측 성공 / 테스트 데이터 실패\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e7788-6aba-4dfb-a1a9-e7b22e08636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit-Learn의 검증 곡선\n",
    "# 다항 회귀(polynomial regression) 모델 사용\n",
    "\"\"\"\n",
    "1차 => y = ax + b\n",
    "2차 => y = ax^2 + bx + c\n",
    "3차 => y = ax^3 + bx^2 + cx + d\n",
    "...\n",
    "\n",
    "SciKit-Learn에서는 이것을 다항식 전처리 프로그램과 결합한 간단한 선형 회구로 구현\n",
    "연산을 하나로 묶는 파이프라인을 사용\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71834fca-8400-459b-97aa-7d96a9bd7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa78f4-2310-45c1-88ca-3aee1931c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_data(N, err=1.0, rseed=1):\n",
    "    # 임의로 데이터 표본 만들기\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    X = rng.rand(N, 1) ** 2\n",
    "    y = 10-1./(X.ravel()+0.1)\n",
    "    if err > 0:\n",
    "        y += err * rng.randn(N)\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fccc0-41db-44be-868a-7190aac64c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터와 함께 여러 차수의 다항식 적합을 시각화\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # 플롯 형식 지정\n",
    "\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, np.newaxis]\n",
    "\n",
    "plt.scatter(X.ravel(), y, color=\"black\")\n",
    "axis = plt.axis()\n",
    "for degree in [1, 3, 5]:\n",
    "    y_test = PolynomialRegression(degree).fit(X, y).predict(X_test)\n",
    "    plt.plot(X_test.ravel(), y_test, label=\"degree{0}\".format(degree))\n",
    "plt.xlim(-0.1, 1.0)\n",
    "plt.ylim(-2, 12)\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386f999-fb6e-4370-8f50-fb82a089064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score, val_score = validation_curve(PolynomialRegression(), X, y,\n",
    "                                          param_name=\"polynomialfeatures_degree\", param_range=degree, cv=7)\n",
    "plt.plot(degree, np.median(train_score, 1), color=\"blue\", label=\"training score\")\n",
    "plt.plot(degree, np.median(val_score, 1), color=\"red\", label=\"validation score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e10e9a-4d40-4a03-a4c4-f911ddbb8fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 곡선 => 3차 다항식이 최적의 트레이드오프\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_test = PolynomialRegression(3).fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9b425-0ab8-4821-8aeb-f98d71d5c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선\n",
    "X2, y2 = make_data(200)\n",
    "plt.scatter(X2.ravel(), y2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84877574-ab4b-45a3-9edc-f1755d30c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_curve\n",
    "degree = np.arange(0, 21)\n",
    "train_score2, val_score2 = validation_curve(PolynomialRegression(), X2, y2,\n",
    "                                          param_name=\"polynomialfeatures_degree\", param_range=degree, cv=7)\n",
    "plt.plot(degree, np.median(train_score2, 1), color=\"blue\", label=\"training score\")\n",
    "plt.plot(degree, np.median(val_score2, 1), color=\"red\", label=\"validation score\")\n",
    "plt.plot(degree, np.median(train_score, 1), color=\"blue\", alpha=.3, linestyle=\"dashed\")\n",
    "plt.plot(degree, np.median(val_score, 1), color=\"red\", alpha=.3, linestyle=\"dashed\")\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(\"score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b37f1-7b4b-4d03-ba25-8f429f9e69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 큰 규모의 데이터세트가 훨씬 더 복잡한 모델을 지원할 수 있다\n",
    "# 검증 곡선의 행동은 두개의 입력값에 의해 결정 => 1. 모델 복잡도 2. 훈련 데이터 점의 개수\n",
    "# 훈련 집합의 크기에 따른 훈련 점수/검증 점수의 플롯을 학습 곡선(learning curve) 이라고 한다\n",
    "\"\"\"\n",
    "해당 복잡도의 모델은 작은 데이터세트를 과적합 => 훈련점수는 높지만 검증점수는 낮다\n",
    "해당 복잡도의 모델은 큰 데이터세트를 과소적합 => 훈련점수는 감소하지만 검증점수는 증가한다\n",
    "\n",
    "특정 모델이 수렴할 만큼 충분한 데이터 점을 가지게 되면 훈련 데이터를 더 늘리는 것은 도움 X\"\n",
    "=> 더 복잡한 모델을 사용할 것\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d2d8f-cd7f-4b59-9008-b7b583f70e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SciKit-Learn의 학습 곡선\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve(PolynomialRegression(degree), X, y, cv=7,\n",
    "                                         train_sizes=np.linspace(0.3, 1, 25))\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color=\"blue\", label=\"training score\")\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color=\"red\", label=\"validation score\")\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1], color=\"gray\",\n",
    "                 linestyle=\"dashed\")\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel(\"training size\")\n",
    "    ax[i].set_ylabel(\"score\")\n",
    "    ax[i].set_title(\"degree = {0}\".format(degree), size=14)\n",
    "    ax[i].legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10f6ab-4e8a-424a-b0a9-3f30a410a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터를 더 많이 추가한다고 해서 적합도가 눈에 띄게 개선되지 않을 것이다\n",
    "# 더 복잡한 모델로 옮겨가면 수렴 점수는 높아지지만 모델편차는 높아지는 것을 감수해야 한다.\n",
    "# => 월씬 더 많은 데이터를 추가하면 더욱 복잡한 모델의 학습 곡선도 결국에는 수렴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e02b9c8-648c-4ae2-ac68-d795f81ccc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 검증: 그리드 검색\n",
    "# grid_search 모듈\n",
    "\n",
    "# 자동 그리드 검색을 통해 결정된최적합 모델\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\"polynomialfeatures_degree\": np.arange(21),\n",
    "              \"linearregression_fit_intercept\": [True, False],\n",
    "              \"linearregression_normalize\": [True, False]}\n",
    "\n",
    "grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e2662-06ec-4327-b6e3-319fe9968f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X, y);\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2be158-1bf3-4055-ab1c-84563479fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "lim = plt.axis()\n",
    "y_text = model.fit(X, y).predict(X_test)\n",
    "plt.plot(X_test.ravel(), y_test, hold=True);\n",
    "plt.axis(lim);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef412665-b247-4eef-a052-e4ded115db7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
